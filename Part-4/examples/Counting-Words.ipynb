{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because what's a parallel computing demo without counting words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some utilitiles for excluding commmon phrases and normalizing words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "non_word = re.compile(r'[\\W\\d]+', re.UNICODE)\n",
    "\n",
    "def normalize_word(word):\n",
    "    \"\"\"normalize a word\n",
    "    \n",
    "    simply strips non-word characters and case\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    word = non_word.sub('', word)\n",
    "    return word\n",
    "\n",
    "common_words = {\n",
    "'the','of','and','in','to','a','is','it','that','which','as','on','by',\n",
    "'be','this','with','are','from','will','at','you','not','for','no','have',\n",
    "'i','or','if','his','its','they','but','their','one','all','he','when',\n",
    "'than','so','these','them','may','see','other','was','has','an','there',\n",
    "'more','we','footnote', 'who', 'had', 'been',  'she', 'do', 'what',\n",
    "'her', 'him', 'my', 'me', 'would', 'could', 'said', 'am', 'were', 'very',\n",
    "'your', 'did', 'not',\n",
    "}\n",
    "\n",
    "def yield_words(filename):\n",
    "    \"\"\"returns a generator of words in a file\"\"\"\n",
    "    import io\n",
    "    with io.open(filename, errors='replace') as f:\n",
    "        for line in f:\n",
    "            for word in line.split():\n",
    "                word = normalize_word(word)\n",
    "                if word:\n",
    "                    yield word\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function that reads a file, and returns a dictionary\n",
    "with string keys of phrases of `n` words,\n",
    "whose values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams(filename, n=1):\n",
    "    \"\"\"compute ngram counts for the contents of a file\"\"\"\n",
    "    word_iterator = yield_words(filename)\n",
    "    counts = {}\n",
    "    def _count_gram(gram):\n",
    "        common = sum(word in common_words for word in gram)\n",
    "        if common > n / 2.0:\n",
    "            # don't count ngrams that are >= 50% common words\n",
    "            return\n",
    "        sgram = ' '.join(gram)\n",
    "        counts.setdefault(sgram, 0)\n",
    "        counts[sgram] += 1\n",
    "    \n",
    "    gram = []\n",
    "    \n",
    "    # get the first word\n",
    "    while len(gram) < n:\n",
    "        try:\n",
    "            word = next(word_iterator)\n",
    "            if not word:\n",
    "                continue\n",
    "        except StopIteration:\n",
    "            return counts\n",
    "        else:\n",
    "            gram.append(word)\n",
    "    \n",
    "    _count_gram(gram)\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            word = next(word_iterator)\n",
    "        except StopIteration:\n",
    "            break\n",
    "        else:\n",
    "            gram.append(word)\n",
    "            gram.pop(0)\n",
    "            _count_gram(gram)\n",
    "    return counts\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile cathat.txt\n",
    "the cat in the hat is a cat whose hat is big."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams('cathat.txt', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams('cathat.txt', 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fetch some interesting data from Project Gutenberg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    from urllib.request import urlretrieve # py3\n",
    "except ImportError:\n",
    "    from urllib import urlretrieve # py2\n",
    "\n",
    "davinci_url = \"http://www.gutenberg.org/files/5000/5000-8.txt\"\n",
    "\n",
    "if not os.path.exists('davinci.txt'):\n",
    "    # download from project gutenberg\n",
    "    print(\"Downloading Da Vinci's notebooks from Project Gutenberg\")\n",
    "    urlretrieve(davinci_url, 'davinci.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "def print_common(freqs, n=10):\n",
    "    \"\"\"Print the n most common keys by count.\"\"\"\n",
    "    words, counts = freqs.keys(), freqs.values()\n",
    "    items = zip(counts, words)\n",
    "    items = sorted(items, reverse=True)\n",
    "    justify = 0\n",
    "    for (count, word) in items[:n]:\n",
    "        justify = max(justify, len(word))\n",
    "    \n",
    "    for (count, word) in items[:n]:\n",
    "        print(word.rjust(justify), count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the serial version\n",
    "print(\"Serial word frequency count:\")\n",
    "%time counts = ngrams('davinci.txt', 1)\n",
    "print_common(counts, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the davinci.txt into one file per engine:\n",
    "text = open('davinci.txt', encoding='latin1', errors='replace').read()\n",
    "lines = text.splitlines()\n",
    "nlines = len(lines)\n",
    "n = 10\n",
    "\n",
    "block = nlines//n\n",
    "for i in range(n):\n",
    "    chunk = lines[i*block:(i+1)*(block)]\n",
    "    with open('davinci%i.txt' % i, 'w') as f:\n",
    "        f.write('\\n'.join(chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.path.abspath(os.getcwd())\n",
    "fnames = [ os.path.join(cwd, 'davinci%i.txt' % i) for i in range(n)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipyparallel as ipp\n",
    "rc = ipp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view = rc.load_balanced_view()\n",
    "eall = rc[:]\n",
    "eall.push(dict(\n",
    "    non_word=non_word,\n",
    "    yield_words=yield_words,\n",
    "    common_words=common_words,\n",
    "    normalize_word=normalize_word,\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: parallel ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a version of ngrams that runs in parallel,\n",
    "rejoining the results into a single count dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ngrams_parallel(view, fnames, n=1):\n",
    "    \"\"\"Compute ngrams in parallel\n",
    "    \n",
    "    view - An IPython View\n",
    "    fnames - The filenames containing the split data.\n",
    "    \"\"\"\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load ../soln/ngrams.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parallel ngrams\")\n",
    "%time pcounts = ngrams_parallel(view, fnames, 3)\n",
    "print_common(pcounts, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A bit more data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download some Project Gutenberg samples from ntlk (avoid rate-limiting on PG itself)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_samples = 'http://nltk.github.com/nltk_data/packages/corpora/gutenberg.zip'\n",
    "if not os.path.isdir('gutenberg'):\n",
    "    if not os.path.exists('gutenberg.zip'):\n",
    "        urlretrieve(gutenberg_samples, 'gutenberg.zip')\n",
    "    !unzip gutenberg.zip\n",
    "\n",
    "import glob\n",
    "gutenberg_files = glob.glob(os.path.abspath(os.path.join('gutenberg', '*.txt')))\n",
    "# remove the bible, because it's too big relative to the rest\n",
    "gutenberg_files.remove(os.path.abspath(os.path.join('gutenberg', 'bible-kjv.txt')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls gutenberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Parallel ngrams across several books\")\n",
    "%time pcounts = ngrams_parallel(view, gutenberg_files, 3)\n",
    "print()\n",
    "print_common(pcounts, 10)\n",
    "pcounts = ngrams_parallel(view, gutenberg_files, 4)\n",
    "print()\n",
    "print_common(pcounts, 10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  },
  "widgets": {
   "state": {},
   "version": "1.1.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
